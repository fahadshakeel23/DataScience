{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEKzQwbCA9Yf/rk9NRh1jt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahadshakeel23/DataScience/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "1SltQkacaSNm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from tensorflow.keras import layers, Sequential\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDb reviews dataset from TensorFlow Datasets\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "train_data, test_data = dataset['train'], dataset['test']\n",
        "\n",
        "print(info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNZYkPxBba9e",
        "outputId": "149c877d-8b87-414e-c916-3a1dc8604f13"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='imdb_reviews',\n",
            "    full_name='imdb_reviews/plain_text/1.0.0',\n",
            "    description=\"\"\"\n",
            "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
            "    classification containing substantially more data than previous benchmark\n",
            "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
            "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
            "    \"\"\",\n",
            "    config_description=\"\"\"\n",
            "    Plain text\n",
            "    \"\"\",\n",
            "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
            "    data_dir='/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=80.23 MiB,\n",
            "    dataset_size=129.83 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
            "        'text': Text(shape=(), dtype=string),\n",
            "    }),\n",
            "    supervised_keys=('text', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    nondeterministic_order=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
            "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "      month     = {June},\n",
            "      year      = {2011},\n",
            "      address   = {Portland, Oregon, USA},\n",
            "      publisher = {Association for Computational Linguistics},\n",
            "      pages     = {142--150},\n",
            "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "    }\"\"\",\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Text Preprocessing and Tokenization"
      ],
      "metadata": {
        "id": "R76ZUJZxe-ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define text vectorization layer for tokenizing and indexing words\n",
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length= sequence_length\n",
        ")\n",
        "\n",
        "# adapt vectoize layer to train data text\n",
        "train_text = train_data.map(lambda text, label:text)\n",
        "vectorize_layer.adapt(train_text)\n"
      ],
      "metadata": {
        "id": "mSwOC6qAfAIW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Prepare Dataset for Training"
      ],
      "metadata": {
        "id": "lRQkiKz_f68J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function mapping raw text and label to vectorized text and label\n",
        "def vectorize_text(text, label):\n",
        "    # Remove extra dimension if exists (safe measure)\n",
        "    text = tf.squeeze(text)\n",
        "    # Vectorize text to integer sequences\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "# Apply the vectorization function to datasets\n",
        "train_data = train_data.map(vectorize_text)\n",
        "test_data = test_data.map(vectorize_text)\n",
        "\n",
        "# Batch datasets and prefetch for performance\n",
        "batch_size = 32\n",
        "train_data = train_data.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_data = test_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "L7wHS7ydf7fD"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Build and Compile a Simple Neural Network Model\n",
        "\n"
      ],
      "metadata": {
        "id": "pPmWYBZWgxa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    layers.Embedding(max_features + 1, 128, input_length=sequence_length),  # Embedding layer\n",
        "    layers.GlobalAveragePooling1D(),  # Pooling over the sequence dimension\n",
        "    layers.Dense(64, activation='relu'),  # Hidden fully-connected layer\n",
        "    layers.Dense(1, activation='sigmoid')  # Output sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "sU8JJ9XBgysS",
        "outputId": "ec848a7b-a32e-47a9-c5e2-807695a81a3c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_4      │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_4      │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Train the Model\n",
        "\n"
      ],
      "metadata": {
        "id": "CoKkypjUhxlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 5\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=test_data,\n",
        "    epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "id": "wx5UPjDchyN-",
        "outputId": "8909f52b-0ba0-40e6-a2a5-9cddbfe27aa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 35ms/step - accuracy: 0.6504 - loss: 0.5882 - val_accuracy: 0.8606 - val_loss: 0.3340\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.8795 - loss: 0.2889 - val_accuracy: 0.8668 - val_loss: 0.3194\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - accuracy: 0.9067 - loss: 0.2315 - val_accuracy: 0.8563 - val_loss: 0.3428\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 34ms/step - accuracy: 0.9194 - loss: 0.2075 - val_accuracy: 0.8594 - val_loss: 0.3429\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 38ms/step - accuracy: 0.9261 - loss: 0.1873 - val_accuracy: 0.8529 - val_loss: 0.3793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Evaluate the Model"
      ],
      "metadata": {
        "id": "Qjx37qEZnU3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_data)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "vw6YwLMFnYEe",
        "outputId": "41688db4-c99e-4c61-cf9d-793fe8eb2b22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8525 - loss: 0.3795\n",
            "Test Accuracy: 0.8529\n"
          ]
        }
      ]
    }
  ]
}